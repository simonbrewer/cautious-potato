[
  {
    "objectID": "causal_random_forest.html",
    "href": "causal_random_forest.html",
    "title": "Causal Random Forest",
    "section": "",
    "text": "Background to causal forests based on:\n\nAthey, Susan, Julie Tibshirani, and Stefan Wager. 2019. “Generalized Random Forests.” The Annals of Statistics 47 (2): 1148–78. https://doi.org/10.1214/18-AOS1709.\nWager, Stefan, and Susan Athey. “Estimation and inference of heterogeneous treatment effects using random forests.” Journal of the American Statistical Association 113.523 (2018): 1228-1242.\n\nWill use the grf example to demonstrate this: https://grf-labs.github.io/grf/articles/grf_guide.html"
  },
  {
    "objectID": "causal_random_forest.html#introduction",
    "href": "causal_random_forest.html#introduction",
    "title": "Causal Random Forest",
    "section": "",
    "text": "Background to causal forests based on:\n\nAthey, Susan, Julie Tibshirani, and Stefan Wager. 2019. “Generalized Random Forests.” The Annals of Statistics 47 (2): 1148–78. https://doi.org/10.1214/18-AOS1709.\nWager, Stefan, and Susan Athey. “Estimation and inference of heterogeneous treatment effects using random forests.” Journal of the American Statistical Association 113.523 (2018): 1228-1242.\n\nWill use the grf example to demonstrate this: https://grf-labs.github.io/grf/articles/grf_guide.html"
  },
  {
    "objectID": "causal_random_forest.html#background",
    "href": "causal_random_forest.html#background",
    "title": "Causal Random Forest",
    "section": "Background",
    "text": "Background\nIn causal analysis, we aim to estimate the causal effect \\(\\tau\\) based on a treatment \\(W\\). If data come from a randomized control trial, we assume no confounders, and the effect is just:\n\\[\n\\tau = E[Y_i(1) - Y_i(0)]\n\\]\nIn observational studies, we have confounders (\\(X_i\\)), where we need to account for their effect on both \\(Y_i\\) and \\(W_i\\) for any individual \\(i\\). The effect can now be estimated from the following regression:\n\\[\nY_i = \\tau W_i + \\beta X_i + \\epsilon_i\n\\] Where\n\n\\(\\hat{\\tau}\\) is taken to be a good estimate of \\(\\tau\\). This has the following assumptions:\n\n\nConditional unconfoundedness. i.e. \\(W_i\\) is unconfounded given \\(X_i\\)\n\n\\({Y_i(1), Y_i(0)} \\perp W_i | X_i\\)\n\nThe error is assumed random (conditonal on \\(W_i\\) and \\(X_i\\))\n\n\\(E[\\epsilon_i|X_i,W_i] = 0\\)\n\nThe confounders have a linear effect on \\(Y_i\\)\nThe treatment effect is constant\n\nWe can’t do anything about assumptions 1 and 2 as they are necessary to identify the model. But assumptions 3 and 4 relate to the model used and can be questioned.\n\nNon-linear effects\nFor assumption 3, we can relax this through a standard semi-parametric approach:\n\\[\nY_i = \\tau W_i + f(X_i) + \\epsilon_i\n\\]\nHere, the baseline outcome for individual \\(i\\) is some unknown function of \\(X_i\\), which can be complex. The treatment is still constant and shifts the baseline estimate by \\(\\tau\\).\nThe question is then how to define f() given that it is unknown. To do so, this takes advantage of the residual-on-residual regression (the Frisch-Waugh-Lovell approach). Robinson (1988) showed that this can be used with semi-parametric models. In grf terminology, we define two new objects:\n\nThe propensity score: \\(e(x) = E(W_i|X_i=x)\\)\nThe conditional mean of \\(Y\\): \\(m(x) = E(Y_i|X_i=x) = f(x) + \\tau e(x)\\)\n\nThis can then be rewritten as:\n\\[\nY_i - m(x) = \\tau (W_i - e(x)) + \\epsilon_i\n\\]\nRobinson describes this as ‘centering’: plug in estimates of \\(m(x)\\) and \\(e(x)\\) are obtained, \\(Y_i\\) and \\(W_i\\) are centered,m then the residuals are regressed together.\nIn standard residual-on-residual approaches, we assume that the estimates of \\(m(x)\\) and \\(e(x)\\) are obtained through parametric means (e.g. OLS regression). These are replaced by machine learning models in double machine learning (DML) approaches, including causal RFs. Note that these plug-in estimates are obtained using ‘cross-fitting’. In this, the prediction of, for example, the outcome \\(m(x)\\) based on the confounders \\(X_i\\) for individual \\(i\\) is made with a model trained on all observations except \\(i\\). This avoids bias due to the (different) regularization strategies employed by the two models.\n\n\nNon-constant treatment effects\nIn the original equation, \\(\\tau\\) is assumed to be a constant factor across all individuals. To relax this, they use the idea of subgroups within the data set, each of which has it’s own regression, giving a value of \\(\\tau\\) for each group ($here is still the coefficient of a linear model based on the centered outcome and treatment). The equation now becomes:\n\\[\nY_i = \\tau(X_i) W_i + f(X_i) + \\epsilon_i\n\\]\nwhere \\(\\tau (X_i)\\) is the conditional average treatment effect for a given set of values of \\(X_i\\):\n\\[\nE(Y_i(1) - Y_i(0)|X_i = x)\n\\]\nThe next question is how to find these groups. We want to find subgroups where \\(\\tau\\) can be assumed constant, in other words, we want to find a set of observations that we can calculate the residual-on-residual regression. Note that this is still a linear model, where the slope coefficient gives \\(\\tau\\) for that set of observations:\n\\[\n\\tau(x) = lm(Y_i - \\hat{m}^{-i}(X_i) \\sim W_i - \\hat{e}^{-i}(X_i), \\mbox{weights} = 1(X_i \\in \\square (x)))\n\\]\nA better way of thinking about this may be that we are trying to build a random forest where the outcome is the slope of a regression line. The loss function prioritizes the biggest difference in slope at any split point."
  },
  {
    "objectID": "causal_random_forest.html#example-1",
    "href": "causal_random_forest.html#example-1",
    "title": "Causal Random Forest",
    "section": "Example 1",
    "text": "Example 1\nExample of fitting a causal forest with a nonlinear relationship between X and \\(\\tau\\):\n\nlibrary(grf)\nlibrary(ggplot2)\n\nCreate some data (X[,1] is a confounder, X[,2] & X[,3] have non-linear impact on outcome)\n\nset.seed(42)\nn &lt;- 2000\np &lt;- 10\nX &lt;- matrix(rnorm(n * p), n, p)\nX_test &lt;- matrix(0, 101, p)\nX_test[, 1] &lt;- seq(-2, 2, length.out = 101)\n\nW &lt;- rbinom(n, 1, 0.4 + 0.2 * (X[, 1] &gt; 0))\nY &lt;- pmax(X[, 1], 0) * W + X[, 2] + pmin(X[, 3], 0) + rnorm(n)\n\nPlot X_1 and Y\n\nplot_df = data.frame(X= X[,1], W = as.factor(W), Y = Y)\nggplot(plot_df, aes(x = X, y= Y, col = W)) +\n  geom_point() +\n  geom_smooth()\n\n`geom_smooth()` using method = 'gam' and formula = 'y ~ s(x, bs = \"cs\")'\n\n\n\n\n\n\n\n\n\nFit causal forest\n\ntau_forest &lt;- causal_forest(X, Y, W)\ntau_forest\n\nGRF forest object of type causal_forest \nNumber of trees: 2000 \nNumber of training samples: 2000 \nVariable importance: \n    1     2     3     4     5     6     7     8     9    10 \n0.704 0.037 0.034 0.042 0.033 0.030 0.028 0.025 0.032 0.036 \n\n\nPredict and plot\n\ntau_hat &lt;- predict(tau_forest, X_test)\nplot_df = data.frame(X = rep(X_test[,1], 2),\n                     tau = c(pmax(0, X_test[, 1]), tau_hat$predictions),\n                     label = rep(c(\"Truth\",\"Pred\"), each = nrow(X_test)))\nggplot(plot_df, aes(x = X, y = tau, col = label)) +\n  geom_line() +\n  theme_bw()"
  },
  {
    "objectID": "causal_random_forest.html#example-1b",
    "href": "causal_random_forest.html#example-1b",
    "title": "Causal Random Forest",
    "section": "Example 1b",
    "text": "Example 1b\nThis uses the same data as before, but walks through a single split in a causal tree.\nStep 1: nuisance model for treatment (the propensity model)\n\nforest_W &lt;- regression_forest(X, W, tune.parameters = \"all\")\nW_hat &lt;- predict(forest_W)$predictions\n\nStep 2: nusiance model for outcome\n\nforest_Y &lt;- regression_forest(X, Y, tune.parameters = \"all\")\nY_hat &lt;- predict(forest_Y)$predictions\n\nStep 2b (optional): variable selection for forest\n\nforest_Y_varimp &lt;- variable_importance(forest_Y)\nforest_Y_varimp\n\n             [,1]\n [1,] 0.064545274\n [2,] 0.722531655\n [3,] 0.179488680\n [4,] 0.003975335\n [5,] 0.004832954\n [6,] 0.005150976\n [7,] 0.004071005\n [8,] 0.005604137\n [9,] 0.005134106\n[10,] 0.004665877\n\n\n\ntau_forest &lt;- causal_forest(X, Y, W,\n                            W.hat = W_hat, Y.hat = Y_hat,\n                            tune.parameters = \"all\")\n\ntau_hat &lt;- predict(tau_forest, X_test)\n\nplot(X_test[, 1], tau_hat$predictions, ylim = range(tau_hat$predictions, 0, 2), xlab = \"x\", ylab = \"tau\", type = \"l\")\nlines(X_test[, 1], pmax(0, X_test[, 1]), col = 2, lty = 2)\n\n\n\n\n\n\n\n\n\nlibrary(animation)\n\nbreaks = seq(-2,2,by = 0.1)\nnbreaks = length(breaks)\ntau_df = data.frame(brks = breaks, \n                    tau1 = rep(NA, nbreaks),\n                    tau2 = rep(NA, nbreaks),\n                    dtau = rep(NA, nbreaks))\nplot_df = data.frame(X = X[,1], \n                     W = as.factor(W), \n                     W_hat = W_hat,\n                     Y = Y,\n                     Y_hat = Y_hat)\nplot_df$tau_hat &lt;- predict(tau_forest)$predictions\n\n\n\nOutput at: test.gif\n\n\n[1] TRUE"
  },
  {
    "objectID": "causal_random_forest.html#example-2",
    "href": "causal_random_forest.html#example-2",
    "title": "Causal Random Forest",
    "section": "Example 2",
    "text": "Example 2\nFrom grf docs\n\nIn this section, we walk through an example application of GRF. The data we are using is from Bruhn et al. (2016), which conducted an RCT in Brazil in which high schools were randomly assigned a financial education program (in settings like this it is common to randomize at the school level to avoid student-level interference). This program increased student financial proficiency on average. Other outcomes are considered in the paper, we’ll focus on the financial proficiency score here. A processed copy of this data, containing student-level data from around 17 000 students, is stored on the github repo, it extracts basic student characteristics, as well as additional baseline survey responses we use as covariates (two of these are aggregated into an index by the authors to assess student’s ability to save, and their financial autonomy).\n\n\nlibrary(grf)\n\ndata &lt;- read.csv(\"./data/bruhn2016.csv\")\nY &lt;- data$outcome\nW &lt;- data$treatment\nschool &lt;- data$school\nX &lt;- data[-(1:3)]\n\nAround 30% have one or more missing covariates, the missingness pattern doesn’t seem to vary systematically between the treated and controls, so we’ll keep them in the analysis since GRF supports splitting on X’s with missing values.\n\nsum(!complete.cases(X)) / nrow(X)\n\n[1] 0.2934852\n\n\n\nt.test(W ~ !complete.cases(X))\n\n\n    Welch Two Sample t-test\n\ndata:  W by !complete.cases(X)\nt = -0.3923, df = 9490.1, p-value = 0.6948\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -0.01963191  0.01308440\nsample estimates:\nmean in group FALSE  mean in group TRUE \n          0.5131730           0.5164467 \n\n\nFitting causal forest\n\ncf &lt;- causal_forest(X, Y, W, W.hat = 0.5, clusters = school)"
  }
]