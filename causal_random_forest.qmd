---
title: "Causal Random Forest"
format:
  html:
    toc: true
    embed-resources: true
html-math-method: 
  method: mathjax
date: last-modified
editor: visual
---

## Introduction

Background to causal forests based on:

-   Athey, Susan, Julie Tibshirani, and Stefan Wager. 2019. "Generalized Random Forests." The Annals of Statistics 47 (2): 1148â€“78. https://doi.org/10.1214/18-AOS1709.
-   Wager, Stefan, and Susan Athey. "Estimation and inference of heterogeneous treatment effects using random forests." Journal of the American Statistical Association 113.523 (2018): 1228-1242.

Will use the **grf** example to demonstrate this: https://grf-labs.github.io/grf/articles/grf_guide.html

## Background

In causal analysis, we aim to estimate the causal effect $\tau$ based on a treatment $W$. If data come from a randomized control trial, we assume no confounders, and the effect is just:

$$
\tau = E[Y_i(1) - Y_i(0)]
$$

In observational studies, we have confounders ($X_i$), where we need to account for their effect on both $Y_i$ and $W_i$ for any individual $i$. The effect can now be estimated from the following regression:

$$
Y_i = \tau W_i + \beta X_i + \epsilon_i
$$ Where

-   $\hat{\tau}$ is taken to be a good estimate of $\tau$. This has the following assumptions:

1.  Conditional unconfoundedness. i.e. $W_i$ is unconfounded given $X_i$
    -   ${Y_i(1), Y_i(0)} \perp W_i | X_i$
2.  The error is assumed random (conditonal on $W_i$ and $X_i$)
    -   $E[\epsilon_i|X_i,W_i] = 0$
3.  The confounders have a linear effect on $Y_i$
4.  The treatment effect is constant

We can't do anything about assumptions 1 and 2 as they are necessary to identify the model. But assumptions 3 and 4 relate to the model used and can be questioned.

### Non-linear effects

For assumption 3, we can relax this through a standard semi-parametric approach:

$$
Y_i = \tau W_i + f(X_i) + \epsilon_i
$$

Here, the baseline outcome for individual $i$ is some unknown function of $X_i$, which can be complex. The treatment is still constant and shifts the baseline estimate by $\tau$.

The question is then how to define `f()` given that it is unknown. To do so, this takes advantage of the residual-on-residual regression (the Frisch-Waugh-Lovell approach). Robinson (1988) showed that this can be used with semi-parametric models. In **grf** terminology, we define two new objects:

-   The propensity score: $e(x) = E(W_i|X_i=x)$
-   The conditional mean of $Y$: $m(x) = E(Y_i|X_i=x) = f(x) + \tau e(x)$

This can then be rewritten as:

$$
Y_i - m(x) = \tau (W_i - e(x)) + \epsilon_i
$$

Robinson describes this as 'centering': plug in estimates of $m(x)$ and $e(x)$ are obtained, $Y_i$ and $W_i$ are centered,m then the residuals are regressed together.

In standard residual-on-residual approaches, we assume that the estimates of $m(x)$ and $e(x)$ are obtained through parametric means (e.g. OLS regression). These are replaced by machine learning models in double machine learning (DML) approaches, including causal RFs. Note that these plug-in estimates are obtained using 'cross-fitting'. In this, the prediction of, for example, the outcome $m(x)$ based on the confounders $X_i$ for individual $i$ is made with a model trained on all observations *except* $i$. This avoids bias due to the (different) regularization strategies employed by the two models.

### Non-constant treatment effects

In the original equation, $\tau$ is assumed to be a constant factor across all individuals. To relax this, they use the idea of subgroups within the data set, each of which has it's own regression, giving a value of $\tau$ for each group (\$\tau here is still the coefficient of a linear model based on the centered outcome and treatment). The equation now becomes:

$$
Y_i = \tau(X_i) W_i + f(X_i) + \epsilon_i
$$

where $\tau (X_i)$ is the *conditional* average treatment effect for a given set of values of $X_i$: 

$$
E(Y_i(1) - Y_i(0)|X_i = x)
$$

The next question is how to find these groups. We want to find subgroups where $\tau$ can be assumed constant, in other words, we want to find a set of observations that we can calculate the residual-on-residual regression. Note that this is still a linear model, where the slope coefficient gives $\tau$ *for that set of observations*:

$$
\tau(x) = lm(Y_i - \hat{m}^{-i}(X_i) \sim W_i - \hat{e}^{-i}(X_i), \mbox{weights} = 1(X_i \in \square (x)))
$$


## Example 1


```{r}
data <- read.csv("./data/bruhn2016.csv")
Y <- data$outcome
W <- data$treatment
school <- data$school
X <- data[-(1:3)]

# Around 30% have one or more missing covariates, the missingness pattern doesn't seem
# to vary systematically between the treated and controls, so we'll keep them in the analysis
# since GRF supports splitting on X's with missing values.
sum(!complete.cases(X)) / nrow(X)
#> [1] 0.29
t.test(W ~ !complete.cases(X))
#> 
#>  Welch Two Sample t-test
#> 
#> data:  W by !complete.cases(X)
#> t = -0.4, df = 9490, p-value = 0.7
#> alternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0
#> 95 percent confidence interval:
#>  -0.020  0.013
#> sample estimates:
#> mean in group FALSE  mean in group TRUE 
#>                0.51                0.52
```